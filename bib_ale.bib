@article{saito1997partial,
  title={Partial BFGS update and efficient step-length calculation for three-layer neural networks},
  author={Saito, Kazumi and Nakano, Ryohei},
  journal={Neural Computation},
  volume={9},
  number={1},
  pages={123--141},
  year={1997},
  publisher={MIT Press}
}
@article{TSD,
author = {Tripathi, Subarna and Dane, Gokce and Kang, Byeongkeun and Bhaskaran, Vasudev and Nguyen, Truong},
year = {2017},
month = {05},
pages = {},
title = {LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems}
}

@book{carr2010shallows,
  title={The Shallows: What the Internet is Doing to Our Brains},
  author={Carr, N.G.},
  isbn={9780393072228},
  lccn={2010007639},
  url={https://books.google.it/books?id=9-8jnjgYrgYC},
  year={2010},
  publisher={W.W. Norton}
}
@misc{stanford_history,
    author = "",
    title = {{History of neural networks}},
    howpublished = {\url{https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/index.html}},
    note = {Online; accessed 28 Febraury 2019} ,
    year=2010,
}

@misc{ wiki:caffe2,
    author = "Wikipedia contributors",
    title = "Caffe (software)",
    year = "2019",
    url = "https://en.wikipedia.org/wiki/Caffe_(software)",
    note = "[Online; accessed 1-March-2019]"
}

@misc{ wiki:tipi,
    author = "{Wikipedia contributors}",
    title = "Types of artificial neural networks --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2019",
    url = "https://en.wikipedia.org/w/index.php?title=Types_of_artificial_neural_networks&oldid=885109562",
    note = "[Online; accessed 1-March-2019]"
  }
@article{SCHMIDHUBER201585,
	title = "Deep learning in neural networks: An overview",
	journal = "Neural Networks",
	volume = "61",
	pages = "85 - 117",
	year = "2015",
	issn = "0893-6080",
	doi = "https://doi.org/10.1016/j.neunet.2014.09.003",
	url = "http://www.sciencedirect.com/science/article/pii/S0893608014002135",
	author = "JÃ¼rgen Schmidhuber",
	keywords = "Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation",
	abstract = "In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks."
}
  
@book{nielsen_book,
  title={Neural Networks and Deep Learning},
  author={Michael A. Nielsen},
  isbn={},
  lccn={},
  url={http://neuralnetworksanddeeplearning.com/},
  year={2015},
  publisher={Determination Press}
}
@article{brito2016gpu,
  title={GPU-enabled back-propagation artificial neural network for digit recognition in parallel},
  author={Brito, Ricardo and Fong, Simon and Cho, Kyungeun and Song, Wei and Wong, Raymond and Mohammed, Sabah and Fiaidhi, Jinan},
  journal={The Journal of Supercomputing},
  volume={72},
  number={10},
  pages={3868--3886},
  year={2016},
  publisher={Springer}
}
@misc{ quora:why_gpu,
    author = "{Tim Dettmers}",
    title = "Answer to why gpu are well suited for deep learning?",
    year = "2016",
    url = "https://www.quora.com/Why-are-GPUs-well-suited-to-deep-learning",
    note = "[Online; accessed 10-March-2019]"
  }

@misc{ TF:architecture,
    author = "{}",
    title = "TensorFlow Architecture",
    year = "2019",
    url = "https://www.tensorflow.org/guide/extend/architecture",
    note = "[Online; accessed 11-March-2019]"
  }
  
@misc{ wiki:tf,
   author = "Wikipedia",
   title = "TensorFlow --- Wikipedia{,} L'enciclopedia libera",
   year = "2018",
   url = "http://it.wikipedia.org/w/index.php?title=TensorFlow&oldid=101735405",
   note = "[Online; in data 14-marzo-2019]"
  }
  
%Articolo di O'reilly su pytorch.
@misc{ oreilly:pytorch,
    author = "Mo Patel",
    title = "When two trends fuse: PyTorch and recommender systems",
    year = "2017",
    url = "https://www.oreilly.com/ideas/when-two-trends-fuse-pytorch-and-recommender-systems",
    note = "[Online; accessed accessed 13-March-2019"
  }
@misc{caffe_announcement,
    author = "The Facebook team",
    year="2017",
    title = "Caffe2 Open Source Brings Cross Platform Machine Learning Tools to Developers",
    url = "https://caffe2.ai/blog/2017/04/18/caffe2-open-source-announcement.html",
    note = "[Online; accessed 14-March-2019]"
  }
@misc{caffe_c++,
    author = "The Facebook team",
    year="2017",
    title = "Caffe2 with C++",
    url = "https://caffe2.ai/docs/cplusplus_tutorial.html",
    note = "[Online; accessed 14-March-2019]"
  }  

@misc{caffe_intro,
    author = "The Facebook team",
    year="2017",
    title = "What is Caffe2?",
    url = "https://caffe2.ai/docs/caffe-migration.html",
    note = "[Online; accessed 14-March-2019]"
  }  
    
@misc{ pytorch:docs,
    title = "End-to-end Deep Learning Platform",
    url = "https://pytorch.org/features",
    note = "[Online; accessed 14-March-2019]"
  }  
  
@misc{ oreilly:pytorch_intro,
    author = "Ben Lorica",
    title = "Why AI and machine learning researchers are beginning to embrace PyTorch",
    year = "2017",
    url = "https://www.oreilly.com/ideas/why-ai-and-machine-learning-researchers-are-beginning-to-embrace-pytorch",
    note = "[Online; accessed 13-March-2019]"
  }

% COME SONO ARRIVATO A QUESTA FONTE:
% https://www.quora.com/What-are-the-advantages-of-using-Leaky-Rectified-Linear-Units-Leaky-ReLU-over-normal-ReLU-in-deep-learning

  
@misc{ cs23,
    author = "Andrej Karpathy",
    title = "Biological motivation and connections",
    url = "http://cs231n.github.io/neural-networks-1/#bio",
    note = "[Online; accessed 15-March-2019]"
  }
  
@misc{ quora:relu,
    author = "Prasoon Goyal",
    title = "An answer to the question 'Why is ReLU the most common activation function used in neural networks?'",
    year = "2018",
    url = "https://www.quora.com/Why-is-ReLU-the-most-common-activation-function-used-in-neural-networks",
    note = "[Online; accessed 14-March-2019]"
  }  
  
% QUORA SU MXNET  
@misc{ quora:mxnet,
    author = "Jean-Louis Queguiner",
    title = "An answer to the question 'How is MxNET still surviving as a framework in front of Tensorflow?'",
    year = "2018",
    url = "https://www.quora.com/How-is-MxNET-still-surviving-as-a-framework-in-front-of-Tensorflow",
    note = "[Online; accessed 15-March-2019]"
  }
  
% MARUTI PER MXNET
@misc{ maruti:mxnet,
    title = "Top 8 Deep Learning Frameworks",
    year = "2018",
    url = "https://www.marutitech.com/top-8-deep-learning-frameworks/",
    note = "[Online; accessed 15-March-2019]"
  }  

@misc{ fortune:mxnet,
	author = "Barb Darrow",
    title = "Amazon Has Chosen This Framework to Guide Deep Learning Strategy",
    year = "2016",
    url = "http://fortune.com/2016/11/22/amazon-deep-learning-mxnet/",
    note = "[Online; accessed 15-March-2019]"
  }  
  