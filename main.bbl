\begin{thebibliography}{10}

\bibitem{pytorch:docs}
End-to-end deep learning platform.
\newblock [Online; accessed 14-March-2019].

\bibitem{stanford_history}
{History of neural networks}.
\newblock
  \url{https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/index.html},
  2010.
\newblock Online; accessed 28 Febraury 2019.

\bibitem{maruti:mxnet}
Top 8 deep learning frameworks, 2018.
\newblock [Online; accessed 15-March-2019].

\bibitem{brito2016gpu}
Ricardo Brito, Simon Fong, Kyungeun Cho, Wei Song, Raymond Wong, Sabah
  Mohammed, and Jinan Fiaidhi.
\newblock Gpu-enabled back-propagation artificial neural network for digit
  recognition in parallel.
\newblock {\em The Journal of Supercomputing}, 72(10):3868--3886, 2016.

\bibitem{broomhead1988radial}
David~S Broomhead and David Lowe.
\newblock Radial basis functions, multi-variable functional interpolation and
  adaptive networks.
\newblock Technical report, Royal Signals and Radar Establishment Malvern
  (United Kingdom), 1988.

\bibitem{carr2010shallows}
N.G. Carr.
\newblock {\em The Shallows: What the Internet is Doing to Our Brains}.
\newblock W.W. Norton, 2010.

\bibitem{wiki:caffe2}
Wikipedia contributors.
\newblock Caffe (software), 2019.
\newblock [Online; accessed 1-March-2019].

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in neural information processing systems}, pages
  2672--2680, 2014.

\bibitem{quora:relu}
Prasoon Goyal.
\newblock An answer to the question 'why is relu the most common activation
  function used in neural networks?', 2018.
\newblock [Online; accessed 14-March-2019].

\bibitem{hebb1961organization}
Donald~O Hebb.
\newblock {\em The organization of behavior}.
\newblock na, 1961.

\bibitem{hopfield1982neural}
John~J Hopfield.
\newblock Neural networks and physical systems with emergent collective
  computational abilities.
\newblock {\em Proceedings of the national academy of sciences},
  79(8):2554--2558, 1982.

\bibitem{cs23}
Andrej Karpathy.
\newblock Biological motivation and connections.
\newblock [Online; accessed 15-March-2019].

\bibitem{kohonen1972correlation}
Teuvo Kohonen.
\newblock Correlation matrix memories.
\newblock {\em IEEE transactions on computers}, 100(4):353--359, 1972.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, Patrick Haffner, et~al.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{oreilly:pytorch_intro}
Ben Lorica.
\newblock Why ai and machine learning researchers are beginning to embrace
  pytorch, 2017.
\newblock [Online; accessed 13-March-2019].

\bibitem{mcculloch1943logical}
Warren~S McCulloch and Walter Pitts.
\newblock A logical calculus of the ideas immanent in nervous activity.
\newblock {\em The bulletin of mathematical biophysics}, 5(4):115--133, 1943.

\bibitem{nielsen_book}
Michael~A. Nielsen.
\newblock {\em Neural Networks and Deep Learning}.
\newblock Determination Press, 2015.

\bibitem{quora:mxnet}
Jean-Louis Queguiner.
\newblock An answer to the question 'how is mxnet still surviving as a
  framework in front of tensorflow?', 2018.
\newblock [Online; accessed 15-March-2019].

\bibitem{saito1997partial}
Kazumi Saito and Ryohei Nakano.
\newblock Partial bfgs update and efficient step-length calculation for
  three-layer neural networks.
\newblock {\em Neural Computation}, 9(1):123--141, 1997.

\bibitem{caffe_announcement}
The~Facebook team.
\newblock Caffe2 open source brings cross platform machine learning tools to
  developers, 2017.
\newblock [Online; accessed 14-March-2019].

\bibitem{caffe_c++}
The~Facebook team.
\newblock Caffe2 with c++, 2017.
\newblock [Online; accessed 14-March-2019].

\bibitem{caffe_intro}
The~Facebook team.
\newblock What is caffe2?, 2017.
\newblock [Online; accessed 14-March-2019].

\bibitem{quora:why_gpu}
{Tim Dettmers}.
\newblock Answer to why gpu are well suited for deep learning?, 2016.
\newblock [Online; accessed 10-March-2019].

\bibitem{TSD}
Subarna Tripathi, Gokce Dane, Byeongkeun Kang, Vasudev Bhaskaran, and Truong
  Nguyen.
\newblock Lcdet: Low-complexity fully-convolutional neural networks for object
  detection in embedded systems.
\newblock 05 2017.

\bibitem{networks_zoo}
Fjodor van Veen.
\newblock The neural network zoo.
\newblock
  \url{https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/index.html},
  2016.
\newblock Online; accessed 17 Marzo 2019.

\bibitem{widrow1960adaptive}
Bernard Widrow and Marcian~E Hoff.
\newblock Adaptive switching circuits.
\newblock Technical report, STANFORD UNIV CA STANFORD ELECTRONICS LABS, 1960.

\bibitem{wiki:tipi}
{Wikipedia contributors}.
\newblock Types of artificial neural networks --- {Wikipedia}{,} the free
  encyclopedia, 2019.
\newblock [Online; accessed 1-March-2019].

\end{thebibliography}
